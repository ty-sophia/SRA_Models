{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FIleuCAjoFD8",
    "outputId": "dd846e1a-fa3f-4c18-906c-04e7250392c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dSpejs19fFvg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Image augmentation - implement this code to avoid overfitting. It is basically a geometric transformation\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "#\"rescale\" is basically feature scaling. we are dividing the values in each pixel by 255 to normalize the values to be within 0,1\n",
    "#\"shear range\" , \"zoom range\", and horizontal flips are the geometric transformations to avoid overfitting. \n",
    "training_set = train_datagen.flow_from_directory('/Users/sophiaty/Desktop/SRA Codes/Imaging_Dataset/Train_Set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "#targetsize refers to the final size of the image\n",
    "#batchsize refers to the number of items in a batch to be processed by CNN. We can do 32 bcause we have thousands of images\n",
    "#class_mode is binary because it is either cat or dog! You need to change this for more option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0dxN7TY6-4UG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "#you only perform the feature scaling by the same value (dividing by 255)\n",
    "#you should not transform your test data because this is a \"Data leak\"\n",
    "test_set = test_datagen.flow_from_directory('/Users/sophiaty/Desktop/SRA Codes/Imaging_Dataset/Test_Set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yhOmEnfAAruT"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential() #this initializes the CNN to be a sequential class not a graph one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MASzPLgZA7na"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu',input_shape=[64,64,3]))\n",
    "#this adds a 2D conv layer.\n",
    "#filter? arbitrary number that was chosen, you can try different number of filters\n",
    "#kernel_size = refers to the dimensions of the filter (3 means 3x3 square))\n",
    "#activation =refers to the type of activation function\n",
    "#input_shape contains 64,64,3 to indicate that the images have been rescaled to be 64x64 that is colored (RGB =3).\n",
    "#input_shape's last arg would =1 if the image is black and white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ePdjzF8SDUjZ"
   },
   "outputs": [],
   "source": [
    "#applies max pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#pool-size arg refers to a 2x2 box that is used to pool information from the previous layer\n",
    "#strides refers to the shift size, in this case two boxes/pixels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pDSKeY-nFPv6"
   },
   "outputs": [],
   "source": [
    "#add second conv layer with max pooling. \n",
    "#input_Shape no longer necessary. We only want that in the first layer to attach CNN to the input images\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "USTbJSbXGFDv"
   },
   "outputs": [],
   "source": [
    "#create a vector that will act as the input to the regular ANN aka full connection layer\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dKRKS41ZGkwi"
   },
   "outputs": [],
   "source": [
    "#this adds a fullly connected layer. There are more units or neurons because we have a more complex system. \n",
    "#we are continuing to use the rectifier function as the activation function\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qmcAZhQURx3b"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=5, activation='softmax'))\n",
    "#this adds an output layer that has 5 neurons with a softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "B-sTwcczTrCC"
   },
   "outputs": [],
   "source": [
    "#this step connects the CNN to optimizer, loss function, and metrics.\n",
    "cnn.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=[tf.keras.metrics.SpecificityAtSensitivity(0.2)])\n",
    "#optimizer adam?\n",
    "#loss function is a categorical crossentropy\n",
    "#metrics: accuracy of prediction is calculated but we also want to assess specficity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X2SU5VtSUdyc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.5691 - specificity_at_sensitivity: 0.9327 - val_loss: 1.5874 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 2/80\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 1.3184 - specificity_at_sensitivity: 0.9808 - val_loss: 1.5458 - val_specificity_at_sensitivity: 0.9231\n",
      "Epoch 3/80\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 1.2964 - specificity_at_sensitivity: 0.9231 - val_loss: 1.4743 - val_specificity_at_sensitivity: 0.9038\n",
      "Epoch 4/80\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 1.2164 - specificity_at_sensitivity: 0.9615 - val_loss: 1.4372 - val_specificity_at_sensitivity: 0.9423\n",
      "Epoch 5/80\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 1.2065 - specificity_at_sensitivity: 0.9808 - val_loss: 1.4455 - val_specificity_at_sensitivity: 0.9038\n",
      "Epoch 6/80\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 1.1875 - specificity_at_sensitivity: 0.9808 - val_loss: 1.4562 - val_specificity_at_sensitivity: 0.9038\n",
      "Epoch 7/80\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 1.1835 - specificity_at_sensitivity: 0.9615 - val_loss: 1.4628 - val_specificity_at_sensitivity: 0.9038\n",
      "Epoch 8/80\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 1.1408 - specificity_at_sensitivity: 0.9808 - val_loss: 1.4742 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 9/80\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 1.1648 - specificity_at_sensitivity: 0.9615 - val_loss: 1.4826 - val_specificity_at_sensitivity: 0.9038\n",
      "Epoch 10/80\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 1.0794 - specificity_at_sensitivity: 0.9904 - val_loss: 1.4752 - val_specificity_at_sensitivity: 0.9231\n",
      "Epoch 11/80\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 1.0521 - specificity_at_sensitivity: 0.9904 - val_loss: 1.4670 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 12/80\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.9740 - specificity_at_sensitivity: 1.0000 - val_loss: 1.4816 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 13/80\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 0.9461 - specificity_at_sensitivity: 0.9904 - val_loss: 1.4904 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 14/80\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.9298 - specificity_at_sensitivity: 0.9904 - val_loss: 1.5066 - val_specificity_at_sensitivity: 0.9231\n",
      "Epoch 15/80\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.8475 - specificity_at_sensitivity: 1.0000 - val_loss: 1.5602 - val_specificity_at_sensitivity: 0.9423\n",
      "Epoch 16/80\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 0.8113 - specificity_at_sensitivity: 1.0000 - val_loss: 1.5527 - val_specificity_at_sensitivity: 0.9038\n",
      "Epoch 17/80\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 0.7453 - specificity_at_sensitivity: 0.9904 - val_loss: 1.5592 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 18/80\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 0.7624 - specificity_at_sensitivity: 1.0000 - val_loss: 1.5603 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 19/80\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.6956 - specificity_at_sensitivity: 1.0000 - val_loss: 1.6054 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 20/80\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.6800 - specificity_at_sensitivity: 1.0000 - val_loss: 1.6899 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 21/80\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 0.6864 - specificity_at_sensitivity: 0.9904 - val_loss: 1.8749 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 22/80\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 0.5935 - specificity_at_sensitivity: 1.0000 - val_loss: 1.8154 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 23/80\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.5490 - specificity_at_sensitivity: 1.0000 - val_loss: 1.8240 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 24/80\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.5312 - specificity_at_sensitivity: 1.0000 - val_loss: 1.9415 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 25/80\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 0.4973 - specificity_at_sensitivity: 1.0000 - val_loss: 2.3019 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 26/80\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.4929 - specificity_at_sensitivity: 1.0000 - val_loss: 2.2668 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 27/80\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.4217 - specificity_at_sensitivity: 1.0000 - val_loss: 2.3489 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 28/80\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.4245 - specificity_at_sensitivity: 1.0000 - val_loss: 2.5951 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 29/80\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.3673 - specificity_at_sensitivity: 1.0000 - val_loss: 2.6794 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 30/80\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 0.3136 - specificity_at_sensitivity: 1.0000 - val_loss: 2.7873 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 31/80\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.2659 - specificity_at_sensitivity: 1.0000 - val_loss: 2.7647 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 32/80\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 0.3915 - specificity_at_sensitivity: 1.0000 - val_loss: 2.8954 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 33/80\n",
      "1/1 [==============================] - 1s 500ms/step - loss: 0.3093 - specificity_at_sensitivity: 1.0000 - val_loss: 3.0311 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 34/80\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.2147 - specificity_at_sensitivity: 1.0000 - val_loss: 3.2241 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 35/80\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.2421 - specificity_at_sensitivity: 1.0000 - val_loss: 3.3916 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 36/80\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.2247 - specificity_at_sensitivity: 1.0000 - val_loss: 3.7055 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 37/80\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 0.3379 - specificity_at_sensitivity: 1.0000 - val_loss: 3.4522 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 38/80\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 0.2204 - specificity_at_sensitivity: 1.0000 - val_loss: 3.4815 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 39/80\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 0.2930 - specificity_at_sensitivity: 1.0000 - val_loss: 3.5641 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 40/80\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.2202 - specificity_at_sensitivity: 1.0000 - val_loss: 3.8037 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 41/80\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.2184 - specificity_at_sensitivity: 1.0000 - val_loss: 4.1914 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 42/80\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 0.2534 - specificity_at_sensitivity: 1.0000 - val_loss: 4.1794 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 43/80\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 0.1486 - specificity_at_sensitivity: 1.0000 - val_loss: 4.2870 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 44/80\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 0.3107 - specificity_at_sensitivity: 1.0000 - val_loss: 3.9270 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 45/80\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 0.1794 - specificity_at_sensitivity: 1.0000 - val_loss: 3.9978 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 46/80\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 0.2342 - specificity_at_sensitivity: 1.0000 - val_loss: 3.8206 - val_specificity_at_sensitivity: 0.7692\n",
      "Epoch 47/80\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.1535 - specificity_at_sensitivity: 1.0000 - val_loss: 4.2794 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 48/80\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 0.3109 - specificity_at_sensitivity: 1.0000 - val_loss: 4.0298 - val_specificity_at_sensitivity: 0.7885\n",
      "Epoch 49/80\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.0948 - specificity_at_sensitivity: 1.0000 - val_loss: 4.2570 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 50/80\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.2116 - specificity_at_sensitivity: 1.0000 - val_loss: 4.1969 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 51/80\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.2233 - specificity_at_sensitivity: 1.0000 - val_loss: 4.2333 - val_specificity_at_sensitivity: 0.7885\n",
      "Epoch 52/80\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 0.1291 - specificity_at_sensitivity: 1.0000 - val_loss: 4.5359 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 53/80\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 0.2107 - specificity_at_sensitivity: 1.0000 - val_loss: 4.0067 - val_specificity_at_sensitivity: 0.7692\n",
      "Epoch 54/80\n",
      "1/1 [==============================] - 0s 490ms/step - loss: 0.1109 - specificity_at_sensitivity: 1.0000 - val_loss: 3.8793 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 55/80\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.1788 - specificity_at_sensitivity: 1.0000 - val_loss: 3.9494 - val_specificity_at_sensitivity: 0.8077\n",
      "Epoch 56/80\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 0.1741 - specificity_at_sensitivity: 1.0000 - val_loss: 4.1503 - val_specificity_at_sensitivity: 0.7692\n",
      "Epoch 57/80\n",
      "1/1 [==============================] - 0s 476ms/step - loss: 0.1102 - specificity_at_sensitivity: 1.0000 - val_loss: 4.5612 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 58/80\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 0.0963 - specificity_at_sensitivity: 1.0000 - val_loss: 4.6042 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 59/80\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 0.1902 - specificity_at_sensitivity: 1.0000 - val_loss: 4.1851 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 60/80\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.2254 - specificity_at_sensitivity: 1.0000 - val_loss: 3.9974 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 61/80\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 0.2065 - specificity_at_sensitivity: 1.0000 - val_loss: 3.7743 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 62/80\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 0.0907 - specificity_at_sensitivity: 1.0000 - val_loss: 3.8711 - val_specificity_at_sensitivity: 0.7500\n",
      "Epoch 63/80\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.0796 - specificity_at_sensitivity: 1.0000 - val_loss: 4.1674 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 64/80\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 0.0815 - specificity_at_sensitivity: 1.0000 - val_loss: 4.2873 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 65/80\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 0.1419 - specificity_at_sensitivity: 1.0000 - val_loss: 4.3106 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 66/80\n",
      "1/1 [==============================] - 0s 474ms/step - loss: 0.0812 - specificity_at_sensitivity: 1.0000 - val_loss: 4.7402 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 67/80\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 0.1711 - specificity_at_sensitivity: 1.0000 - val_loss: 4.8041 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 68/80\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 0.1316 - specificity_at_sensitivity: 1.0000 - val_loss: 4.8174 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 69/80\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 0.0520 - specificity_at_sensitivity: 1.0000 - val_loss: 5.3244 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 70/80\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.1616 - specificity_at_sensitivity: 1.0000 - val_loss: 4.8122 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 71/80\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 0.1421 - specificity_at_sensitivity: 1.0000 - val_loss: 4.2750 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 72/80\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 0.0640 - specificity_at_sensitivity: 1.0000 - val_loss: 4.3289 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 73/80\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 0.1188 - specificity_at_sensitivity: 1.0000 - val_loss: 4.3330 - val_specificity_at_sensitivity: 0.8462\n",
      "Epoch 74/80\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 0.0872 - specificity_at_sensitivity: 1.0000 - val_loss: 4.3361 - val_specificity_at_sensitivity: 0.8269\n",
      "Epoch 75/80\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 0.0879 - specificity_at_sensitivity: 1.0000 - val_loss: 4.7090 - val_specificity_at_sensitivity: 0.8077\n",
      "Epoch 76/80\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 0.1510 - specificity_at_sensitivity: 1.0000 - val_loss: 5.5796 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 77/80\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 0.2398 - specificity_at_sensitivity: 1.0000 - val_loss: 5.1939 - val_specificity_at_sensitivity: 0.8846\n",
      "Epoch 78/80\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 0.0486 - specificity_at_sensitivity: 1.0000 - val_loss: 4.8763 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 79/80\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 0.0390 - specificity_at_sensitivity: 1.0000 - val_loss: 4.8861 - val_specificity_at_sensitivity: 0.8654\n",
      "Epoch 80/80\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.0800 - specificity_at_sensitivity: 1.0000 - val_loss: 4.8909 - val_specificity_at_sensitivity: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8aa30cab80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 80)\n",
    "#this fits our model to the training set. We are using the test_Set as the validation test, epochs refer to total number of runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SRA_Model_01_Udemy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
